# StupidGPT ğŸ¤“

> ä»¥ä¸‹READMEå†…å®¹ç”±Cursorç”Ÿæˆ ğŸ¤—

ä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„ç®€å•æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å®ç°ã€‚

## ç®€ä»‹

StupidGPT æ˜¯ä¸€ä¸ªä½¿ç”¨ PyTorch å®ç°çš„ç®€åŒ–ç‰ˆ Transformer æ¨¡å‹ï¼Œç”¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚è¯¥é¡¹ç›®æ—¨åœ¨æä¾›ä¸€ä¸ªç®€å•æ˜“æ‡‚çš„ Transformer å®ç°ï¼Œå¸®åŠ©ç†è§£ Transformer æ¶æ„çš„æ ¸å¿ƒæ¦‚å¿µã€‚

## åŠŸèƒ½

- åŸºäº Transformer æ¶æ„çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
- æ”¯æŒè®­ç»ƒå’Œæ¨ç†åŠŸèƒ½
- ä½¿ç”¨ç®€å•çš„åˆ†è¯å™¨è¿›è¡Œæ–‡æœ¬å¤„ç†
- æ”¯æŒ GPU åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
- æ¨¡å‹å‚æ•°å¯é…ç½®

## è¯´æ˜

1. å…‹éš†é¡¹ç›®åˆ°æœ¬åœ°ï¼š
```bash
git clone [é¡¹ç›®åœ°å€]
cd stupidGPT
```

2. åˆ›å»ºå¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰ï¼š
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# æˆ–
.venv\Scripts\activate  # Windows
```

3. å®‰è£…ä¾èµ–ï¼š
```bash
pip install -r requirements.txt
```

## ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒæ¨¡å‹

```python
from main import train

# è®­ç»ƒæ¨¡å‹
train('dataset.txt', epochs=10000)
```

### æ¨¡å‹æ¨ç†

```python
from main import infer

# ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬
infer('dataset.txt', 'The cloud')
```

## é¡¹ç›®ç»“æ„

```
stupidGPT/
â”œâ”€â”€ main.py              # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ requirements.txt     # é¡¹ç›®ä¾èµ–
â”œâ”€â”€ dataset.txt         # è®­ç»ƒæ•°æ®é›†
â”œâ”€â”€ model_*.bin         # æ¨¡å‹å‚æ•°æ–‡ä»¶
â”œâ”€â”€ tokenizer/          # åˆ†è¯å™¨æ¨¡å—
â”œâ”€â”€ transformer/        # Transformer æ¨¡å‹å®ç°
â”œâ”€â”€ utils/             # å·¥å…·å‡½æ•°
â”œâ”€â”€ layers/            # ç¥ç»ç½‘ç»œå±‚å®ç°
â””â”€â”€ mask/              # æ³¨æ„åŠ›æ©ç ç›¸å…³
```

## è¯¦ç»†è¯´æ˜

### Tokenizer æ¨¡å—

Tokenizer æ˜¯æ–‡æœ¬å¤„ç†çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°å­—åºåˆ—ã€‚æœ¬é¡¹ç›®å®ç°äº† `SimpleTokenizer` ç±»ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

#### ä¸»è¦åŠŸèƒ½

1. **è¯æ±‡è¡¨ç®¡ç†**
   - è‡ªåŠ¨æ„å»ºè¯æ±‡è¡¨ï¼ˆvocabularyï¼‰
   - æ”¯æŒåŠ¨æ€æ·»åŠ æ–°è¯
   - ç»´æŠ¤è¯åˆ°IDå’ŒIDåˆ°è¯çš„åŒå‘æ˜ å°„
   - ç‰¹æ®Štokenå¤„ç†ï¼ˆå¦‚ç©ºæ ¼å’Œå¥å·ï¼‰

2. **æ–‡æœ¬ç¼–ç ä¸è§£ç **
   - å°†æ–‡æœ¬è½¬æ¢ä¸ºtoken IDåºåˆ—
   - æ”¯æŒæœ€å¤§é•¿åº¦é™åˆ¶
   - æ”¯æŒå¡«å……ï¼ˆpaddingï¼‰
   - å°†token IDåºåˆ—è½¬æ¢å›æ–‡æœ¬

3. **è®­ç»ƒæ•°æ®å¤„ç†**
   - æ”¯æŒæ‰¹é‡é‡‡æ ·
   - è‡ªåŠ¨å¤„ç†ç¼–ç å™¨è¾“å…¥ã€è§£ç å™¨è¾“å…¥å’Œç›®æ ‡è¾“å‡º
   - æ”¯æŒåºåˆ—é•¿åº¦å¯¹é½

#### ä¸»è¦æ–¹æ³•

1. **åˆå§‹åŒ–ä¸é…ç½®**
   ```python
   tokenizer = SimpleTokenizer()
   ```

2. **æ„å»ºè¯æ±‡è¡¨**
   ```python
   tokenizer.fit_on_texts(texts)  # textsä¸ºæ–‡æœ¬åˆ—è¡¨
   ```

3. **æ–‡æœ¬ç¼–ç **
   ```python
   # ç¼–ç æ–‡æœ¬ï¼Œæ”¯æŒæœ€å¤§é•¿åº¦å’Œå¡«å……
   token_ids = tokenizer.encode(text, max_length=12, pad=True)
   ```

4. **æ–‡æœ¬è§£ç **
   ```python
   # å°†token IDåºåˆ—è½¬æ¢å›æ–‡æœ¬
   text = tokenizer.decode(token_ids)
   ```

5. **è®­ç»ƒæ•°æ®é‡‡æ ·**
   ```python
   # è·å–è®­ç»ƒæ‰¹æ¬¡æ•°æ®
   enc_inputs, dec_inputs, outputs = tokenizer.random_sample(dataset, batch_size=16, seq_len=12)
   ```

#### ç‰¹æ®Šå¤„ç†

1. **ç‰¹æ®ŠToken**
   - ç©ºæ ¼ï¼ˆID: 0ï¼‰
   - å¥å·ï¼ˆID: 1ï¼‰
   - å…¶ä»–è¯ä»ID 2å¼€å§‹åˆ†é…

2. **å¤§å°å†™å¤„ç†**
   - æ‰€æœ‰è¯éƒ½ä¼šè¢«è½¬æ¢ä¸ºå°å†™
   - ä¿æŒè¯æ±‡è¡¨çš„ä¸€è‡´æ€§

3. **å¡«å……å¤„ç†**
   - ä½¿ç”¨ç©ºæ ¼ï¼ˆID: 0ï¼‰è¿›è¡Œå¡«å……
   - æ”¯æŒæˆªæ–­è¿‡é•¿çš„åºåˆ—

#### ä½¿ç”¨ç¤ºä¾‹

```python
from tokenizer.tokenizer import SimpleTokenizer

# åˆå§‹åŒ–tokenizer
tokenizer = SimpleTokenizer()

# è®­ç»ƒæ•°æ®
texts = ["Hello world", "This is a test"]

# æ„å»ºè¯æ±‡è¡¨
tokenizer.fit_on_texts(texts)

# ç¼–ç æ–‡æœ¬
encoded = tokenizer.encode("Hello world", max_length=5, pad=True)
# è¾“å‡º: [token_id1, token_id2, 0, 0, 0]

# è§£ç 
decoded = tokenizer.decode(encoded)
# è¾“å‡º: "hello world"
```

### Mask æ¨¡å—

Mask æ¨¡å—è´Ÿè´£å¤„ç† Transformer æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æ©ç ï¼Œä¸»è¦åŒ…æ‹¬ä¸‰ç§ç±»å‹çš„æ©ç ï¼š

#### 1. Padding Mask

ç”¨äºå±è”½è¾“å…¥åºåˆ—ä¸­çš„å¡«å……éƒ¨åˆ†ï¼Œç¡®ä¿æ¨¡å‹ä¸ä¼šå…³æ³¨å¡«å……çš„ tokenã€‚

```python
def create_padding_mask(seq, pad_token=0):
    """
    åˆ›å»º Padding Mask
    :param seq: è¾“å…¥åºåˆ—ï¼Œå½¢çŠ¶ï¼š(batch_size, seq_len)
    :param pad_token: å¡«å……çš„TokenIdï¼Œé»˜è®¤ä¸º0
    :return: PaddingMask (batch_size, 1, 1, seq_len)
    """
```

#### 2. Causal Mask

ç”¨äºç¡®ä¿è§£ç å™¨åœ¨é¢„æµ‹å½“å‰ä½ç½®æ—¶çœ‹ä¸åˆ°æœªæ¥çš„è¯ï¼Œå®ç°è‡ªå›å½’ç”Ÿæˆã€‚

```python
def create_causal_mask(seq_len):
    """
    åˆ›å»ºCausalMask
    :param seq_len: åºåˆ—é•¿åº¦
    :return: (1, seq_len, seq_len)
    """
```

#### 3. Combine Mask

å°† Padding Mask å’Œ Causal Mask ç»„åˆåœ¨ä¸€èµ·ï¼Œç”¨äºè§£ç å™¨çš„æ³¨æ„åŠ›è®¡ç®—ã€‚

```python
def create_combine_mask(padding_mask, causal_mask):
    """
    ç»„åˆPaddingMaskå’ŒCausalMask
    :param padding_mask: PaddingMask
    :param causal_mask: CausalMask
    :return: CombineMask
    """
```

### Transformer æ¨¡å—

Transformer æ¨¡å—å®ç°äº†å®Œæ•´çš„ Transformer æ¶æ„ï¼ŒåŒ…æ‹¬ç¼–ç å™¨ã€è§£ç å™¨å’Œç›¸å…³ç»„ä»¶ã€‚

#### 1. æ¨¡å‹é…ç½®

é€šè¿‡ `TransformerConfig` ç±»é…ç½®æ¨¡å‹å‚æ•°ï¼š

```python
config = TransformerConfig(
    batch_size=16,
    vocab_size=1200,
    embedding_dim=16,
    num_heads=8,
    hidden_dim=16,
    num_encoder_layers=16,
    num_decoder_layers=16,
    max_len=12,
    dropout=0.1
)
```

#### 2. ç¼–ç å™¨ï¼ˆEncoderï¼‰

ç¼–ç å™¨ç”±å¤šä¸ª `EncoderLayer` ç»„æˆï¼Œæ¯ä¸ªå±‚åŒ…å«ï¼š
- å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
- å‰é¦ˆç¥ç»ç½‘ç»œ
- æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–

```python
class EncoderLayer(nn.Module):
    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0.1):
        self.self_attention = MultiHeadAttentionLayer(embed_dim, num_heads, dropout)
        self.add_and_norm1 = AddAndNormLayer(embed_dim, dropout)
        self.feed_forward = FeedForwardLayer(embed_dim, hidden_dim, dropout)
        self.add_and_norm2 = AddAndNormLayer(embed_dim, dropout)
```

#### 3. è§£ç å™¨ï¼ˆDecoderï¼‰

è§£ç å™¨ç”±å¤šä¸ª `DecoderLayer` ç»„æˆï¼Œæ¯ä¸ªå±‚åŒ…å«ï¼š
- å¸¦æ©ç çš„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
- ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æœºåˆ¶
- å‰é¦ˆç¥ç»ç½‘ç»œ
- æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–

```python
class DecoderLayer(nn.Module):
    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0.1):
        self.self_attention = MultiHeadAttentionLayer(embed_dim, num_heads, dropout)
        self.add_and_norm1 = AddAndNormLayer(embed_dim, dropout)
        self.encoder_decoder_attention = MultiHeadAttentionLayer(embed_dim, num_heads, dropout)
        self.add_and_norm2 = AddAndNormLayer(embed_dim, dropout)
        self.feed_forward = FeedForwardLayer(embed_dim, hidden_dim, dropout)
        self.add_and_norm3 = AddAndNormLayer(embed_dim, dropout)
```

#### 4. å®Œæ•´æ¨¡å‹

`TransformerModel` ç±»æ•´åˆäº†æ‰€æœ‰ç»„ä»¶ï¼š

```python
class TransformerModel(nn.Module):
    def __init__(self, config: TransformerConfig):
        self.enc_embedding = EmbeddingLayer(...)
        self.encoder = nn.Sequential([EncoderLayer(...) for _ in range(config.num_encoder_layers)])
        self.dec_embedding = EmbeddingLayer(...)
        self.decoder = nn.Sequential([DecoderLayer(...) for _ in range(config.num_decoder_layers)])
        self.linear = nn.Linear(...)
```

#### 5. å‰å‘ä¼ æ’­æµç¨‹

1. åˆ›å»ºç¼–ç å™¨å’Œè§£ç å™¨çš„æ©ç 
2. å¯¹è¾“å…¥è¿›è¡ŒåµŒå…¥
3. é€šè¿‡ç¼–ç å™¨å¤„ç†è¾“å…¥åºåˆ—
4. é€šè¿‡è§£ç å™¨ç”Ÿæˆè¾“å‡ºåºåˆ—
5. é€šè¿‡çº¿æ€§å±‚å’Œsoftmaxå¾—åˆ°æœ€ç»ˆè¾“å‡º

## é…ç½®è¯´æ˜

æ¨¡å‹çš„ä¸»è¦é…ç½®å‚æ•°åœ¨ `main.py` ä¸­å®šä¹‰ï¼ŒåŒ…æ‹¬ï¼š
- æ‰¹æ¬¡å¤§å°ï¼ˆbatch_sizeï¼‰
- è¯åº“å¤§å°ï¼ˆvocab_sizeï¼‰
- åµŒå…¥ç»´åº¦ï¼ˆembedding_dimï¼‰
- æ³¨æ„åŠ›å¤´æ•°ï¼ˆnum_headsï¼‰
- éšè—å±‚ç»´åº¦ï¼ˆhidden_dimï¼‰
- ç¼–ç å™¨å±‚æ•°ï¼ˆnum_encoder_layersï¼‰
- è§£ç å™¨å±‚æ•°ï¼ˆnum_decoder_layersï¼‰
- æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆmax_lenï¼‰
- Dropout æ¯”ç‡ï¼ˆdropoutï¼‰

## ä¾èµ–é¡¹

- torch ~= 2.6.0
- numpy
- loguru

## æ³¨æ„äº‹é¡¹

- ç¡®ä¿æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®
- è®­ç»ƒè¿‡ç¨‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå»ºè®®ä½¿ç”¨ GPU åŠ é€Ÿ
- æ¨¡å‹å‚æ•°æ–‡ä»¶ä¼šè‡ªåŠ¨ä¿å­˜ï¼Œå¯ä»¥ç”¨äºåç»­æ¨ç†
- Tokenizer çš„è¯æ±‡è¡¨å¤§å°ä¼šå½±å“æ¨¡å‹æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨
- å»ºè®®åœ¨è®­ç»ƒå‰å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œç¡®ä¿è´¨é‡
- æ³¨æ„è°ƒæ•´æ¨¡å‹å‚æ•°ä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚
- åˆç†è®¾ç½®åºåˆ—é•¿åº¦å’Œæ‰¹æ¬¡å¤§å°ä»¥å¹³è¡¡æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨

## æ•ˆæœ

### è®­ç»ƒ
![](https://github.com/0niros/stupidGPT/doc/image/train.png)

### æ¨ç†
![](https://github.com/0niros/stupidGPT/doc/image/infer.png)