from os.path import exists
import numpy as np
import torch
from torch import nn, optim
from loguru import logger

from tokenizer.tokenizer import SimpleTokenizer
from transformer.transformer_model import TransformerConfig, TransformerModel
from utils.device import choose_device

# Transformer é…ç½®
transformer_config = TransformerConfig(
        # Batch size
        batch_size=16,
        # è¯åº“å¤§å°
        vocab_size=1200,
        # embeddingç»´åº¦
        embedding_dim=16,
        # å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
        num_heads=8,
        # éšè—å±‚ç»´åº¦
        hidden_dim=16,
        # ç¼–ç å™¨å±‚æ•°
        num_encoder_layers=16,
        # è§£ç å™¨å±‚æ•°
        num_decoder_layers=16,
        # æœ€å¤§é•¿åº¦
        max_len=12,
        # dropout
        # 0.1
        dropout=0.1
)

def infer(train_data_file, prompt):
    """
    æ¨¡å‹æ¨ç†
    :param train_data_file: æ¨¡å‹å‚æ•°æ–‡ä»¶
    :param prompt: æç¤ºè¯
    """
    logger.info(f"ğŸš€ å¼€å§‹æ¨ç†, ä½¿ç”¨{choose_device().type}")
    # 1. æ„å»ºæ¨¡å‹
    model = TransformerModel(transformer_config)
    if exists(transformer_config.get_train_file_name()):
        model.load_state_dict(torch.load(transformer_config.get_train_file_name()))
        logger.info(f"âœ… åŠ è½½{transformer_config.get_train_file_name()}æˆåŠŸ")

    # 2. åŠ è½½æ•°æ®é›†
    train_data = None
    tokenizer = SimpleTokenizer()
    with open(train_data_file, 'r') as f:
        train_data = f.readlines()
        train_data = [line.strip() for line in train_data]
    tokenizer.fit_on_texts(train_data)

    # 3. æ¨ç†
    model.eval()
    prompt_encoded = tokenizer.encode(prompt, transformer_config.max_len, True)
    encoder_input = torch.from_numpy(np.array(prompt_encoded.copy())).unsqueeze(0)
    decoder_input = torch.from_numpy(np.array(prompt_encoded.copy())).unsqueeze(0)

    input_index = encoder_input.size(0) + 1
    output_words = []
    for i in range(input_index, transformer_config.max_len - 1):
        encoder_input = encoder_input.to(choose_device())
        decoder_input = decoder_input.to(choose_device())
        output = model(encoder_input, decoder_input)
        output = output.argmax(dim=-1)
        output_next_word = output[0][i]
        decoder_input[0][i] = output_next_word
        output_words.append(output_next_word.item())
        if output_next_word.item() == 1 or output_next_word.item() == 0:
            break
    logger.info(f"âœ… Prompt:{prompt} æ¨ç†å®Œæˆ!")
    logger.info(f"{prompt} {tokenizer.decode(output_words)}")

def train(train_data_file, epochs: int):
    """
    æ¨¡å‹è®­ç»ƒ
    :param train_data_file: æ¨¡å‹å‚æ•°æ–‡ä»¶
    :param epochs: è®­ç»ƒEpoch
    """
    logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒ, ä½¿ç”¨{choose_device().type}")
    # 1. åŠ è½½æ•°æ®é›†
    train_data = None
    tokenizer = SimpleTokenizer()
    with open(train_data_file, 'r') as f:
        train_data = f.readlines()
        train_data = [line.strip() for line in train_data]
    tokenizer.fit_on_texts(train_data)

    # 2. æ„å»ºæ¨¡å‹
    model = TransformerModel(transformer_config)
    model = model.to(choose_device())
    if exists(transformer_config.get_train_file_name()):
        model.load_state_dict(torch.load(transformer_config.get_train_file_name()))
        logger.info(f"âœ… åŠ è½½{transformer_config.get_train_file_name()}æˆåŠŸ")
    model.train()

    # 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.00001)

    # 4. è®­ç»ƒ
    loss_epoch = 0
    for epoch in range(epochs):
        optimizer.zero_grad()

        # 4.1 å–æ•°æ®
        encoder_input, decoder_input, target_output = tokenizer.random_sample(train_data, transformer_config.batch_size, transformer_config.max_len)
        encoder_input = encoder_input.to(choose_device())
        decoder_input = decoder_input.to(choose_device())
        target_output = target_output.to(choose_device())

        # 4.2 å‰å‘ä¼ æ’­
        output = model(encoder_input, decoder_input)
        loss = criterion(output.view(-1, output.shape[-1]), target_output.view(-1))

        # 4.3 åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        loss_epoch += loss.item()

        # 4.4 ä¿å­˜æ•°æ®
        if epoch % 200 == 0 and epoch != 0:
            logger.info(f"Epoch {epoch}/{epochs}, Avg Loss: {loss_epoch / 200:.4f}")
            torch.save(model.state_dict(), transformer_config.get_train_file_name())
            loss_epoch = 0


# è®­ç»ƒ
train('dataset.txt', 600)

# æ¨ç†
#infer('dataset.txt', 'The cloud')